\documentclass[a4]{jpconf}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{url}
\usepackage{xspace}
\usepackage{iopams}
\usepackage[pdftex,
      colorlinks=true,
      urlcolor=blue,       % \href{...}{...} external (URL)
      filecolor=green,     % \href{...} local file
      linkcolor=blue,       % \ref{...} and \pageref{...}
      pagebackref,
      pdfpagemode=UseNone,
      bookmarksopen=true]{hyperref}

\providecommand{\fixme}[1]{\xspace{\sffamily{[\bfseries{}FIXME:} #1]}}


\begin{document}
% paper title
\title{Many-core applications to online track reconstruction in HEP experiments}
% Author list
\author{S.~Amerio$^1$, 
  D.~Bastieri$^1$, 
  M.~Corvo$^1$, 
  A.~Gianelle$^1$, 
  W.~Ketchum$^2$,
  T.~Liu$^3$, 
  A.~Lonardo$^4$, 
  D.~Lucchesi$^1$,
  S.~Poprocki$^5$, 
  R.~Rivera$^3$, 
  L.~Tosoratto$^4$,
  P.~Vicini$^4$
  and 
  P.~Wittich$^5$,
}
\address{$^1$ INFN and University of Padova, Padova, Italy}
\address{$^2$ Los Alamos National Laboratory, Los Alamos, New Mexico, USA}
\address{$^3$ Fermi National Accelerator Laboratory, Batavia, Illinois, USA}
\address{$^4$ INFN Roma, Rome, Italy}
\address{$^5$ Cornell University, Ithaca, New York, USA}

\ead{silvia.amerio@pd.infn.it}

\begin{abstract}
  Interest in parallel architectures applied to real time selections is 
  growing in High Energy Physics (HEP) experiments. In this paper we describe
  performance measurements of Graphic Processing Units (GPUs) and Intel Many
  Integrated Core architecture (MIC) when applied to a typical HEP online task: 
  the selection of events based on the trajectories of charged particles.
  We use as benchmark a scaled-up version of the algorithm used at CDF 
  experiment at Tevatron for online track reconstruction - the SVT algorithm - 
as a realistic  test-case for low-latency trigger systems using new computing
  architectures for LHC experiment. We examine the
  complexity/performance trade-off in porting existing serial
  algorithms to many-core devices. Measurements of both data processing 
  and data transfer latency are shown, considering different I/O strategies 
  to/from the parallel devices. 
\end{abstract}

\section{Introduction}
%%% Verbatim from NSS paper
Real-time event reconstruction plays a fundamental role in High Energy
Physics (HEP) experiments at hadron colliders.  Reducing the rate of
data to be saved on tape from millions to hundreds of events per
second is critical. To increase the purity of the collected samples,
rate reduction has to be coupled with an initial selection of the most
interesting events.  In a typical hadron collider experiment, the
event rate has to be reduced from tens of MHz to a few kHz.  The
selection system (trigger) is usually organized in multiple levels,
each capable of performing a finer selection on more complex physics
objects describing the event. Trigger systems usually comprise a first
level based on custom hardware, followed by one or two levels usually
based on farms of general purpose processors.  The possibility of also
using commercial devices at a low trigger level is very appealing:
they are subject to continuous performance improvements driven by the
consumer market, are less expensive than dedicated hardware, and are
easier to support.  Among the commercial devices, many-core architectures 
such as Graphic Processing Units (GPUs)~\cite{bib_gpu} and Intel Many
  Integrated Core (MIC)~\cite{bib_intelMIC} are of particular interest for 
online 
selections given their great computing power (the latest 
\textsc{nvidia}~\cite{bib_nvidia} GPU architecture, Kepler, exceeds Teraflop 
computing power); moreover,
high-level programming architectures based on C/C++ such as
\textsc{cuda}~\cite{bib_cuda}  and 
\textsc{opencl}~\cite{bib_opencl} make these devices more accessible to 
the general user.  The ultimate
goal of this study is to investigate the strengths and weaknesses of
many-core devices when applied in a low latency environment, with particular
emphasis on the data transfer latency to/from the device and the
algorithm latency for processing on the device in a manner similar to a
typical HEP trigger application, and to understand the cost/complexity
ratio of porting legacy serial code to many-core devices.

We showed initial studies on GPU performance in low-latency
environments ($\sim 100~\mu$s) in  previous
papers~\cite{TIPP2011,NSS2012}.  In this paper we extend those studies
to include other many-core systems (Intel MIC in addition to GPUs).
The algorithm run on the parallel architecture is a complete
version of the fast track fitting algorithm of the Silicon Vertex
Tracker (SVT) system at CDF~\cite{SVT1}.
%We also compare the performance of GPU
%cards from different manufacturers (AMD vs NVIDIA) and different
%computing environments (\textsc{cuda} and \textsc{opencl}) on the same
%card. 
Starting with a serial algorithm
implemented on a CPU, we test an \textit{embarrassingly parallel}
algorithm on the Intel MIC environment. In this case each event is
handled independently by a core on the accelerator, and the
parallelization is achieved with only minor changes to the legacy code
and is only possible in the Intel MIC environment. Next we consider an
algorithm where we unroll three internal nested loops and run these in
parallel on a GPU, using the \textsc{cuda} environment. This second
approach is programmatically more complicated and less trivial to
implement. In neither case have we re-thought the basic algorithms or
the data structures used. To achieve optimal performance, these steps
would have to be taken.  As one might expect, the improvement from the
first approach is rather modest, and the second approach shows larger
performance gains. For GPUs, we also test different strategies to transfer
 data to/from the device.

\section{SVT track fitting algorithm}
The Silicon Vertex Trigger (SVT)~\cite{SVT1,SVT2} is a 
track reconstruction processor used at 
trigger level in the CDF experiment at Tevatron accelerator. It reconstructs 
tracks in about 20 $\mu s$ in time in two steps: for each event 
first low resolution tracks (\textit{roads}) are found among the hits left 
in the tracking detector by charged particles; then track fitting is 
performed on all possible combinations of hits inside a road. 
This algorithm uses a linearized approximation to track-fitting as 
implemented in hardware (described in greater detail in~\cite{SVT3}). 
With the SVT approach, the determination of the track parameters 
($p_i$) is reduced to a simple scalar product:
\[
p_i = \vec{f_i} \cdot \vec{x_i} + q_i,
\]
where $\vec{x_i}$ are input silicon hits, and $\vec{f_i}$ and $q_i$ are 
pre-defined constant sets. For each set of hits, the algorithm
computes the impact parameter $d_0$, the azimuthal angle $\phi$, 
the transverse momentum $p_\mathrm{T}$ , and the $\chi^2$ of the
fitted track by using simple operations such as memory lookup and 
integer addition and multiplication.

We ported the track fitting as it is well suited to parallelization -
each track can be handled independently.

\subsection{Code implementation}
The starting point of of our studies is the SVT track fitting
simulation code, written in the \textsc{c} language. SVT track fitting
is divided into three main functions: the unpacking of input data and
filling of all the necessary arrays, followed by the computation of
all possible combinations of hits in each road and finally the fit of
each combination. Three main loops are present - on events, roads and
hit combinations.  To be run on GPUs, the code has been ported to
\textsc{cuda}: each step - unpack, combine and track fit - is
performed by a specific kernel; the three nested loops are unrolled so
that each GPU thread processes a single combination of hits.  To run
on MIC, we adopted the so-called \textit{embarrassingly parallel}
approach and used \textsc{pragma OpenMP} for statements to unroll the external
loop on the events, so that each core processes a single event: the
porting requires much less effort compared to \textsc{cuda}, but the level of
parallelism is limited.

\section{Experimental setup and data flow}
The many-core devices used in this study are listed in
Table~\ref{tab_hwspecs}. The GPUs include a less expensive gaming
class GPU (GTX) and ones optimized for scientific computing
(Tesla). The MIC corresponds to a Xeon Phi introduced in November 2012.

\begin{table}[!t]
  \centering
  \begin{tabular}{|l|c|c|c|c|}
    \hline
    Model & Tesla M2050 & Tesla K20m & GeForce GTX  Titan & MIC 5110P \\
    \hline
    \hline
    Performance (SP, GFlops) & 1030 & 3520 & 4500 & 2022 \\
    Memory bandwidth  (GB/s) & 148 & 208 & 288  & 320\\   
    Memory size (GB) & 3 & 5 & 6 & 8 \\
    Number of cores & 448 & 2496 & 2688 & 240 \\
    Clock speed (GHz) & 1.15 & 0.706 & 0.837 & 1.053 \\
    \hline
  \end{tabular}
  \caption{Capabilities of the many-core devices used in this study,
    according to the manufacturer's specifications. The first three
    are \textsc{nvidia} GPUs and the final is an Intel Xeon Phi.}
  \label{tab_hwspecs}
\end{table}

To measure the data transfer latency we use a computing cluster composed of 
12 identical nodes.  Each node contains a Intel Xeon E6520 2.4 GHz 
CPU and two Tesla M2075 GPU cards. The nodes are connected by InfiniBand 
communication links using Connect-X2 Mellanox or Apenet+ adapters. 
Apenet+ is an FPGA-based PCIe board supporting peer-to-peer communication with 
Tesla and Kepler cards~\cite{apenet2010}.
Two nodes of this cluster are used to measure data transfer latency,
one acting as a transmitter and the
other as a receiver.  Data are transferred from the transmitter to the 
receiver, processed on the GPU and sent back to the receiver (see
Fig.~\ref{fig:data_flow}).  
The latency for a complete loop is measured on the transmitter using 
standard C libraries.
\begin{figure}[tbph]
\centering
\includegraphics[width=3.5in]{figures/SetUp-general}
\caption{Data flow. Data is sent from the transmitter PC to the
  receiver PC, where it is processed by the GPU before being returned
  to the transmitter PC. The transmitter plays the role of the
  detector as the source of the data and as an upstream trigger
  processor as the data's ultimate sink. The receiver PC plays the
  role of a component in the trigger system. }
\label{fig:data_flow}
\end{figure}
In this setup, the transmitter can represent the detector, as
the source of the data, or an upstream trigger processor, as
the ultimate sink of the data, while the the receiver is the
trigger system: the time to transfer data to the receiver is thus a
rough estimate of the latency to transfer the data from the detector
front-end to the trigger system.

\section{Results}
We use as input data events with a fixed number of roads and combinations: 
each event has 2048 combinations to be fitted. To explore different data 
taking conditions the number of events ranges from one to 3000, i.e. from 2048 
to about six millions of combinations to fit.
 
\subsection{Data processing}
Each data sample is processed 100 times by the track fitting algorithm. 
The average latency as a function of the number of fits is presented in 
Fig.~\ref{fig:algo_only_timing} for the serial,
embarrassingly parallel and parallel algorithms. We see that the
embarrassingly parallel algorithm gives a modest increase with respect
to the serial (CPU) algorithm. Switching to a fully parallel algorithm
affords a much more significant speed improvement. 
Accelerators performance drop with decreasing number of fits, as can be seen 
in Fig.~\ref{fig:algo_only_timing_zoom}.
Fig.~\ref{fig:algo_only_speedup} shows the speed-up with respect to the serial 
algorithm run on a standard CPU (Intel Xeon E5630): the maximum gain is 
obtained processing at least 500 events. This means that to fully exploit 
parallel architectures millions of fits have to be performed in parallel.  


%\begin{figure*}[!t]
%\centering
%\subfigure[]
%{\label{fig:algo_only_timing}
%\includegraphics[width=3in]{figures/TimeComp_MIC.pdf}}
%\hspace{1mm}
%\subfigure[]
%{\label{fig_algo_only_timing_zoom}
%\includegraphics[width=3in]{figures/TimeCompZoom_MIC.pdf}}
%\caption{breakdown}
%\end{figure*}
%

 \begin{figure}[!tbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/TimeComp_MIC}
    \caption{Algorithm-only comparison for timing as a function of the
      number of track fits. We compare timing on CPUs (serial), Intel
      MIC (embarrassingly parallel), and GPUs (fully parallel), in
      blue, greed, and red, respectively. The GPUs exhibit the best
      performance due to the full parallelization. }
    \label{fig:algo_only_timing}
  \end{figure}
 
  \begin{figure}[!tbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/TimeCompZoom_MIC.png} 
    \caption{Algorithm-only comparison for timing as a function of the
      number of track fits: zoom in the low number of fits region. At
      low number of fits, the CPU performs better, due to start-up
      costs associated with data transfers.}
    \label{fig:algo_only_timing_zoom}
  \end{figure}
 
 \begin{figure}[!tbp]
   \centering
   \includegraphics[width=0.9\linewidth]{figures/Speedup_MIC}
   \caption{Speed-up with respect to a standard CPU (Intel Xeon
     E5630). The speed-ups plateau after about two million fits.
     %for
     %the GPUs (in red) while the Intel MIC still shows a small slope
     %beyond that point. 
     \fixme{Silvia: removed the comment about the different GPU/MI beahviour. These are the speedup values for the MIC: 0.07, 0.49, 1.79, 2.25, 2.32, 3.35, 4.38,4.74, 5.36, 5.16, 5.34 }}
   \label{fig:algo_only_speedup}
 \end{figure}


\subsubsection{Breakdown of computing time}
In Fig.~\ref{fig:breakdown} we show the fractional time spent in
various parts of the algorithm for the embarrassingly parallel algorithm 
(on Intel MIC) and the parallel algorithm (on NVIDIA Titan GPU), as a 
function of the number of fits. On both accelerator cards the fractional times
are constant over 500 input events, where computing resources are saturated. 
Unlike the MIC, the fit stage takes most of the time on the GPU: this 
could be caused by the intense memory access frequency intrinsic to
this part of the algorithm.  
\begin{figure*}[tbp]
\centering
\subfigure[]
{\label{fig:breakdown_MIC}
\includegraphics[width=0.45\linewidth]{figures/Mic5_perc_nk}}
\subfigure[]
{\label{fig:breakdown_GPU}
\includegraphics[width=0.45\linewidth]{figures/titan_perc}}
\caption{Breakdown of computing time for MIC (a) and the GTX Titan GPU
  (b). Red corresponds to unpacking, green corresponds to generating
  hit combinations, blue is the linearized track fit, and magenta
  corresponds to offloading (MIC only). For MIC, combinations and fitting
  take the same amount of time for large number of events. For GPU,
  fitting dominates. }
\label{fig:breakdown}
\end{figure*}

\subsection{Data transfer}
The experimental setup described in Fig.~\ref{fig:data_flow} allows us to 
test different data transfer  strategies to the GPU. The standard data transfer 
strategy is via the system memory, where the 
PCIe adapter card and the GPU allocate \emph{separate} buffers on the
system memory for the copy (as shown in Fig.~\ref{fig:standardDT}).
This is inefficient, as the data are copied twice in the 
system memory before being transferred to the GPU/PCIe card. 
Data may also be transferred using Direct Memory Access (DMA, 
GPUDirect~\cite{bib_GPUDirect}) to the CPU memory:
the PCIe card and the GPU share the \textit{same} buffer on the CPU memory; as 
a result the data are copied only once in the CPU memory 
(Fig.~\ref{fig:GPUDirectV1}). 
With our experimental setup two additional copy strategies can be tested
which are the results of different levels of 
optimization of the GPUDirect protocol:
\begin{itemize}
\item \textsc{cuda}-Aware MPI, where the copy latency is further reduced
by automatically allocating the buffer on the CPU memory;
\item peer-to-peer (P2P) strategy, when data are transferred 
directly to the GPU, without any  
intermediate copy to the CPU (Fig.~\ref{fig:GPUDirectV2}).
\end{itemize}


\begin{figure*}[!t]
\centering
\subfigure[]
{\label{fig:standardDT}
\includegraphics[width=0.3\linewidth]{figures/noGPUDirect}}
\hspace{1mm}
\subfigure[]
{\label{fig:GPUDirectV1}
\includegraphics[width=0.3\linewidth]{figures/GPUDirect}}
\subfigure[]
{\label{fig:GPUDirectV2}
\includegraphics[width=0.3\linewidth]{figures/GPUDirect-APE}}
\caption{Standard data transfer (a), via GPUDirect (b) and via
  GPUDirect with P2P support (c). In (a), two buffers are required in
  the main memory. In GPUDirect (b), one of the main memory buffers is
  eliminated. In GPUDirect with P2P support, data is sent directly
  from the APEnet+ transceiver to the GPU memory.}
\end{figure*}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/datatransfer}
  \caption{Total latency (data transfer, copy to/from the GPU and data
    processing on the GPU) as a function of data buffer sizes, for
    three different levels of optimization of GPUDirect: v1.0,
    \textsc{cuda}-aware MPI and peer-to-peer. The smallest transferred
    data packed is 600 kB. \fixme{Below it says 600 kb is the place where
    P2P gets worse?}}
  \label{fig:xferlatency}
\end{figure}

In Fig.~\ref{fig:xferlatency} we show the total latency (data
transfer, copy to/from the GPU and data processing on the GPU) as a
function of data packet size when data are transferred using GPUDirect
v1.0, \textsc{cuda}-aware MPI and P2P. For the packet sizes over 600
kB \textsc{cuda}-aware MPI gives the best performance (see also~\cite{NSS2012} and~\cite{bib_mvapich})  \fixme{do we have this plot? Silvia: no, and will not have in time for the proceedings... but we can put a reference, see slides after page 47.}.


\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/cudaware}
  \caption{Time per fit, in msec. The two curves show total timing
    with and without calculations performed on the GPU, thereby
    showing the considerable time spent in data transfer. About
    20-25\% of the time is spent in data transfer.}
  \label{fig:transferOnly}
\end{figure}

The data transfer latency accounts for a significant part of the total
latency, as can be seen in Fig.~\ref{fig:transferOnly}: about 20-25\%
of total latency is due to moving the data to and from the GPU.


\section{Conclusions}
We have examined a staged approach to using accelerator cards in a
hadron collider trigger application. We have demonstrated that in this
application, significant gains can be achieved with the
`embarrassingly parallel' approach on an Intel MIC
architecture, with the smallest amount of required changes to an
existing serial code base. However, better performance is achieved
with GPUs and a more complete event-level parallelization using
\textsc{cuda}. 

%% gives unnumbered ack in CHEP style
\ack
The authors would like to thank the Fermilab staff and the FTK group at the 
University of Chicago for their support. This work was supported by the
U.S. Department of Energy, the U.S. National Science Foundation and the Italian
Istituto Nazionale di Fisica Nucleare. 

\section*{References}

%% CHEP2013 recommended style for bibliography
\bibliographystyle{iopart-num}

\bibliography{gpu}



 

\end{document}


